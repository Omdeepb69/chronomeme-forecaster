{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ChronoMeme Forecaster: Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Description:** Predicts the short-term 'virality' or trend score of internet memes based on social media mention frequency and sentiment analysis over time.\n",
        "\n",
        "**Features:**\n",
        "*   Ingests time-series data of meme mentions (using mock data here).\n",
        "*   Applies basic sentiment analysis (simulated).\n",
        "*   Uses a time series model (Prophet) to forecast future mention frequency/trend score.\n",
        "*   Visualizes historical meme popularity and predicted trend.\n",
        "*   Calculates a simple 'peak virality' prediction window."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from prophet import Prophet\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import warnings\n",
        "import random\n",
        "from datetime import timedelta\n",
        "\n",
        "# Settings\n",
        "warnings.filterwarnings('ignore')\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading and Inspection (Using Mock Data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_mock_data(days=180, num_memes=3, freq='H'):\n",
        "    base_date = pd.to_datetime('2023-01-01')\n",
        "    date_rng = pd.date_range(start=base_date, periods=days * 24, freq=freq)\n",
        "    \n",
        "    all_data = []\n",
        "\n",
        "    for i in range(num_memes):\n",
        "        meme_id = f'meme_{i+1}'\n",
        "        \n",
        "        # Simulate trend + seasonality + noise for mentions\n",
        "        time_factor = np.linspace(0, 5 * np.pi, len(date_rng))\n",
        "        trend = (np.sin(time_factor - i * np.pi/2) + 1.1) * (50 + i * 20) # Different peak times\n",
        "        seasonality = 10 * np.sin(2 * np.pi * date_rng.hour / 24) + 5 * np.sin(2 * np.pi * date_rng.dayofweek / 7) # Daily and weekly patterns\n",
        "        noise = np.random.normal(0, 15 + i*5, len(date_rng))\n",
        "        \n",
        "        mention_count = np.maximum(0, trend + seasonality + noise).astype(int)\n",
        "        \n",
        "        # Simulate sentiment (correlated slightly with mentions, with noise)\n",
        "        sentiment_base = (mention_count / mention_count.max()) * 0.6 - 0.3 # Base sentiment related to popularity\n",
        "        sentiment_noise = np.random.normal(0, 0.15, len(date_rng))\n",
        "        sentiment_shift = np.sin(time_factor/3 + i * np.pi) * 0.1 # Slow sentiment shifts\n",
        "        sentiment_score = np.clip(sentiment_base + sentiment_noise + sentiment_shift, -1, 1)\n",
        "        \n",
        "        meme_df = pd.DataFrame({\n",
        "            'timestamp': date_rng,\n",
        "            'meme_id': meme_id,\n",
        "            'mention_count': mention_count,\n",
        "            'sentiment_score': sentiment_score\n",
        "        })\n",
        "        all_data.append(meme_df)\n",
        "        \n",
        "    df = pd.concat(all_data, ignore_index=True)\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    return df\n",
        "\n",
        "# Generate data\n",
        "df = generate_mock_data(days=90, num_memes=3)\n",
        "\n",
        "# Inspect data\n",
        "print(\"Data Head:\")\n",
        "print(df.head())\n",
        "print(\"\\nData Info:\")\n",
        "df.info()\n",
        "print(\"\\nData Description:\")\n",
        "print(df.describe())\n",
        "print(\"\\nMissing Values:\")\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aggregate data daily for clearer visualization\n",
        "df_daily = df.groupby(['meme_id', pd.Grouper(key='timestamp', freq='D')])[['mention_count', 'sentiment_score']].agg(\n",
        "    mention_count=('mention_count', 'sum'),\n",
        "    sentiment_score=('sentiment_score', 'mean')\n",
        ").reset_index()\n",
        "\n",
        "print(\"\\nDaily Aggregated Data Head:\")\n",
        "print(df_daily.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 7))\n",
        "sns.lineplot(data=df_daily, x='timestamp', y='mention_count', hue='meme_id')\n",
        "plt.title('Daily Mention Count Over Time by Meme')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Total Daily Mentions')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 7))\n",
        "sns.lineplot(data=df_daily, x='timestamp', y='sentiment_score', hue='meme_id')\n",
        "plt.title('Average Daily Sentiment Score Over Time by Meme')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Average Sentiment Score')\n",
        "plt.ylim(-1, 1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "sns.histplot(data=df_daily, x='mention_count', hue='meme_id', kde=True, bins=30)\n",
        "plt.title('Distribution of Daily Mention Counts')\n",
        "plt.xlabel('Daily Mention Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "sns.histplot(data=df_daily, x='sentiment_score', hue='meme_id', kde=True, bins=30)\n",
        "plt.title('Distribution of Average Daily Sentiment Scores')\n",
        "plt.xlabel('Average Daily Sentiment Score')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation between mentions and sentiment (using daily data)\n",
        "for meme in df_daily['meme_id'].unique():\n",
        "    meme_data = df_daily[df_daily['meme_id'] == meme]\n",
        "    correlation = meme_data['mention_count'].corr(meme_data['sentiment_score'])\n",
        "    print(f\"Correlation between mentions and sentiment for {meme}: {correlation:.2f}\")\n",
        "    \n",
        "    # Lagged correlation (does sentiment today correlate with mentions tomorrow?)\n",
        "    meme_data['mention_lag-1'] = meme_data['mention_count'].shift(-1)\n",
        "    lagged_corr = meme_data['sentiment_score'].corr(meme_data['mention_lag-1'])\n",
        "    print(f\"Correlation between today's sentiment and tomorrow's mentions for {meme}: {lagged_corr:.2f}\")\n",
        "    \n",
        "    plt.figure(figsize=(8, 5))\n",
        "    sns.scatterplot(data=meme_data, x='mention_count', y='sentiment_score')\n",
        "    plt.title(f'Mention Count vs Sentiment Score for {meme}')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Statistical Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Time Series Decomposition (Example for meme_1)\n",
        "meme1_daily = df_daily[df_daily['meme_id'] == 'meme_1'].set_index('timestamp')['mention_count']\n",
        "\n",
        "# Need at least 2 full periods for seasonal decomposition, let's assume weekly seasonality (period=7)\n",
        "if len(meme1_daily) >= 14:\n",
        "    decomposition = seasonal_decompose(meme1_daily, model='additive', period=7)\n",
        "    \n",
        "    fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(12, 10), sharex=True)\n",
        "    decomposition.observed.plot(ax=ax1)\n",
        "    ax1.set_ylabel('Observed')\n",
        "    decomposition.trend.plot(ax=ax2)\n",
        "    ax2.set_ylabel('Trend')\n",
        "    decomposition.seasonal.plot(ax=ax3)\n",
        "    ax3.set_ylabel('Seasonal')\n",
        "    decomposition.resid.plot(ax=ax4)\n",
        "    ax4.set_ylabel('Residual')\n",
        "    plt.suptitle('Time Series Decomposition for meme_1 (Daily Mentions)')\n",
        "    plt.xlabel('Date')\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Not enough data points for seasonal decomposition with period=7.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stationarity Test (Augmented Dickey-Fuller Test)\n",
        "def adf_test(timeseries, name):\n",
        "    print(f'\\nAugmented Dickey-Fuller Test for {name}:')\n",
        "    # Handle potential NaNs from decomposition or differencing if applied\n",
        "    timeseries_cleaned = timeseries.dropna()\n",
        "    if timeseries_cleaned.empty:\n",
        "        print(\"Series is empty after dropping NaNs, cannot perform ADF test.\")\n",
        "        return\n",
        "    \n",
        "    result = adfuller(timeseries_cleaned)\n",
        "    print('ADF Statistic: %f' % result[0])\n",
        "    print('p-value: %f' % result[1])\n",
        "    print('Critical Values:')\n",
        "    for key, value in result[4].items():\n",
        "        print('\\t%s: %.3f' % (key, value))\n",
        "    if result[1] <= 0.05:\n",
        "        print(\"Result: Reject the null hypothesis (H0). Data is likely stationary.\")\n",
        "    else:\n",
        "        print(\"Result: Fail to reject the null hypothesis (H0). Data is likely non-stationary.\")\n",
        "\n",
        "# Test stationarity for each meme's daily mention count\n",
        "for meme in df_daily['meme_id'].unique():\n",
        "    meme_series = df_daily[df_daily['meme_id'] == meme].set_index('timestamp')['mention_count']\n",
        "    adf_test(meme_series, f'{meme} Daily Mentions')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Feature Engineering Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Work with the daily aggregated data\n",
        "df_feat = df_daily.copy()\n",
        "\n",
        "# Sort data for time-based features\n",
        "df_feat = df_feat.sort_values(by=['meme_id', 'timestamp'])\n",
        "\n",
        "# Lag features (previous day's mentions and sentiment)\n",
        "df_feat['mention_lag_1'] = df_feat.groupby('meme_id')['mention_count'].shift(1)\n",
        "df_feat['sentiment_lag_1'] = df_feat.groupby('meme_id')['sentiment_score'].shift(1)\n",
        "\n",
        "# Rolling window features (e.g., 7-day rolling mean/std)\n",
        "df_feat['mention_roll_mean_7'] = df_feat.groupby('meme_id')['mention_count'].transform(lambda x: x.rolling(window=7, min_periods=1).mean())\n",
        "df_feat['mention_roll_std_7'] = df_feat.groupby('meme_id')['mention_count'].transform(lambda x: x.rolling(window=7, min_periods=1).std())\n",
        "df_feat['sentiment_roll_mean_7'] = df_feat.groupby('meme_id')['sentiment_score'].transform(lambda x: x.rolling(window=7, min_periods=1).mean())\n",
        "\n",
        "# Time-based features\n",
        "df_feat['dayofweek'] = df_feat['timestamp'].dt.dayofweek\n",
        "df_feat['dayofyear'] = df_feat['timestamp'].dt.dayofyear\n",
        "df_feat['weekofyear'] = df_feat['timestamp'].dt.isocalendar().week.astype(int)\n",
        "df_feat['month'] = df_feat['timestamp'].dt.month\n",
        "\n",
        "# Interaction feature (example)\n",
        "df_feat['mention_x_sentiment_lag1'] = df_feat['mention_lag_1'] * df_feat['sentiment_lag_1']\n",
        "\n",
        "# Display features for one meme\n",
        "print(\"\\nFeature Engineering Example (meme_1):\")\n",
        "print(df_feat[df_feat['meme_id'] == 'meme_1'].head(10))\n",
        "\n",
        "# Check for NaNs introduced by lagging/rolling features\n",
        "print(\"\\nNaNs after Feature Engineering:\")\n",
        "print(df_feat.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Initial Model Testing (Prophet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select data for one meme (e.g., meme_1)\n",
        "meme_to_forecast = 'meme_1'\n",
        "df_prophet = df_daily[df_daily['meme_id'] == meme_to_forecast][['timestamp', 'mention_count']].copy()\n",
        "\n",
        "# Prepare data for Prophet (requires columns 'ds' and 'y')\n",
        "df_prophet = df_prophet.rename(columns={'timestamp': 'ds', 'mention_count': 'y'})\n",
        "\n",
        "# Split data for simple validation (e.g., last 14 days for testing)\n",
        "train_cutoff = df_prophet['ds'].max() - timedelta(days=14)\n",
        "df_train = df_prophet[df_prophet['ds'] <= train_cutoff]\n",
        "df_test = df_prophet[df_prophet['ds'] > train_cutoff]\n",
        "\n",
        "print(f\"Training data shape: {df_train.shape}\")\n",
        "print(f\"Test data shape: {df_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize and fit Prophet model\n",
        "# Prophet automatically handles seasonality (yearly, weekly, daily if applicable)\n",
        "model = Prophet(daily_seasonality=False, weekly_seasonality=True, yearly_seasonality=False, \n",
        "                changepoint_prior_scale=0.05) # Adjust prior scale based on trend flexibility needed\n",
        "\n",
        "# Add potential regressors (example: lagged sentiment - requires careful handling of future values)\n",
        "# For simplicity, we'll stick to univariate forecasting first.\n",
        "# df_train_reg = pd.merge(df_train, df_feat[['timestamp', 'meme_id', 'sentiment_lag_1']], \n",
        "#                         left_on='ds', right_on='timestamp', how='left')\n",
        "# model.add_regressor('sentiment_lag_1')\n",
        "\n",
        "model.fit(df_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create future dataframe for predictions (including test period + future forecast)\n",
        "future_periods = 30 # Forecast 14 days of test + 16 extra days\n",
        "future = model.make_future_dataframe(periods=future_periods, freq='D')\n",
        "\n",
        "# Add regressor values to future dataframe if used\n",
        "# future_reg = pd.merge(future, df_feat[['timestamp', 'meme_id', 'sentiment_lag_1']], \n",
        "#                         left_on='ds', right_on='timestamp', how='left')\n",
        "# future_reg = future_reg.ffill() # Simple forward fill for future regressor values (use with caution!)\n",
        "\n",
        "# Make predictions\n",
        "forecast = model.predict(future)\n",
        "\n",
        "# Display forecast results\n",
        "print(\"\\nForecast Data Head:\")\n",
        "print(forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].head())\n",
        "print(\"\\nForecast Data Tail:\")\n",
        "print(forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot forecast\n",
        "fig1 = model.plot(forecast)\n",
        "plt.title(f'Prophet Forecast for {meme_to_forecast} Mentions')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Mention Count')\n",
        "# Add actual test data points to the plot\n",
        "plt.scatter(df_test['ds'], df_test['y'], color='red', s=10, label='Actual Test Data')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot forecast components\n",
        "fig2 = model.plot_components(forecast)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate forecast on the test set\n",
        "forecast_test = forecast[forecast['ds'].isin(df_test['ds'])]\n",
        "\n",
        "mae = mean_absolute_error(df_test['y'], forecast_test['yhat'])\n",
        "rmse = np.sqrt(mean_squared_error(df_test['y'], forecast_test['yhat']))\n",
        "mean_actual = df_test['y'].mean()\n",
        "mape = np.mean(np.abs((df_test['y'] - forecast_test['yhat']) / df_test['y'])) * 100 if mean_actual != 0 else np.inf\n",
        "\n",
        "print(f\"\\nEvaluation Metrics on Test Set ({meme_to_forecast}):\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
        "print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\" if mape != np.inf else \"MAPE: Undefined (zero actual values)\")\n",
        "print(f\"Mean Actual Value: {mean_actual:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate simple 'peak virality' prediction window\n",
        "# Look for the max predicted value in the future forecast period\n",
        "future_forecast = forecast[forecast['ds'] > df_prophet['ds'].max()]\n",
        "\n",
        "if not future_forecast.empty:\n",
        "    peak_forecast_value = future_forecast['yhat'].max()\n",
        "    peak_forecast_date = future_forecast.loc[future_forecast['yhat'].idxmax(), 'ds']\n",
        "    \n",
        "    # Define window around the peak (e.g., +/- 1 day)\n",
        "    peak_window_start = peak_forecast_date - timedelta(days=1)\n",
        "    peak_window_end = peak_forecast_date + timedelta(days=1)\n",
        "    \n",
        "    print(f\"\\nPeak Virality Prediction ({meme_to_forecast}):\")\n",
        "    print(f\"  Predicted Peak Value (yhat): {peak_forecast_value:.2f}\")\n",
        "    print(f\"  Predicted Peak Date: {peak_forecast_date.strftime('%Y-%m-%d')}\")\n",
        "    print(f\"  Simple Peak Window: {peak_window_start.strftime('%Y-%m-%d')} to {peak_window_end.strftime('%Y-%m-%d')}\")\n",
        "    \n",
        "    # Highlight peak on forecast plot\n",
        "    fig = model.plot(forecast)\n",
        "    plt.scatter(df_test['ds'], df_test['y'], color='red', s=10, label='Actual Test Data')\n",
        "    plt.axvline(peak_forecast_date, color='green', linestyle='--', label=f'Predicted Peak Date ({peak_forecast_date.strftime(\"%Y-%m-%d\")})')\n",
        "    plt.axvspan(peak_window_start, peak_window_end, color='green', alpha=0.1, label='Peak Window')\n",
        "    plt.title(f'Prophet Forecast with Predicted Peak for {meme_to_forecast}')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Mention Count')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo future forecast data available to predict peak.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## End of Exploration"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}